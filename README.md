# Research-_Work_1_course
Оглавление
Задача No1: Решение и выводы
Задача No2: Решение и выводы
Задача No3: Решение и выводы
Задача No4: Решение и выводы
Задача No5: Решение и выводы
Задача No6: Решение и выводы
Заключение: Краткая характеристика выводов по каждой задаче
Список использованной литературы (по ГОСТу)
Приложения:
Код для решения задачи No1
Код для решения задачи No2
Код для решения задачи No3
Код для решения задачи No4
Код для решения задачи No5
Код для решения задачи No6
Отчет по задаче No
Задача 1

Необходимо загрузить данные из указанного набора и произвести следующие действия.

Набор данных: Swiss.

Объясняемая переменная: Examination

Регрессоры: Fertility,Education

Оцените среднее значение, дисперсию и СКО переменных Examination, Fertility,
Education
Examination: Среднее арифметическое = 16.48936, Дисперсия = 63.64662, СКО
=7.977883.

Fertility: Среднее арифметическое = 70.14255, Дисперсия = 156.0425, СКО

= 12.4917.

Education: Среднее арифметическое = 10.97872, Дисперсия = 92.45606, СКО
= 9.615407.

Результат посчитанных функций в Приложении 1.

Постройте зависимости вида y = a + bx, где y – объясняемая переменная,
x –регрессор (для каждого варианта по две зависимости). Оцените, насколько
«хороша» модель по коэффициенту детерминации. Оцените, есть ли взаимосвязь
между объясняемой переменной и объясняющей переменной (по значению p-
статистики, «количеству звездочек» у регрессора в модели.
Зависимость Examination~Fertility. R^2 = 40%. Значение коэффициента детерминации
меньше 80%, поэтому заключаем, что линейной зависимости нет. p-
value = 9.45(10*-7) , *** у Fertility означают сильную взаимосвязь Fertility и
Examination. Fertility и Examination связаны отрицательно (-0.41)

Таблица 1.1. Характеристики модели зависимости параметра Examination от параметра
Fertility в наборе данных Swiss.

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимост
и
(Intercept) 45.42289 5.17670 8.774 2.65e- 11 ***
Fertility - 0.41250 0.07268 - 5.675 9.45e- 07 ***
Зависимость Examination~Education. R^2 = 47%. Значение коэффициента детерминации
меньше 80%, поэтому заключаем, что линейной зависимости нет. p-

value = 4.811(10*-8) , *** у Education означают сильную взаимосвязь Education и
Examination. Education и Examina tion связаны положительно (0.57)

Таблица 1.2. Характеристики модели зависимости параметра Examination от параметра
Education в наборе данных Swiss.

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимост
и
(Intercept) 10.12748 1.28589 7.876 5.23e- 10 ***
Education 0.57947 0.08852 6.546 4.81e- 08 ***
Код решения задачи в Приложении 1.

Во время практики вы освоили расчет среднего арифметического, дисперсии и
среднеквадратического отклонения с использованием встроенных функций. Эти
статистические метрики позволяют получить информацию о распределении данных и
их разбросе.

Относительно зависимости между переменными, вы сделали вывод, что показатель
"Examination" не имеет линейной зависимости от переменных "Education" и "Fertility".

Education и Examination взаимосвязаны положительно (0.57)

Отчет по задаче No
Задача 2

Необходимо загрузить данные из указанного набора и произвести следующие действия.

Набор данных: Mtcars.

Объясняемая переменная: mpg.

Регрессоры: cyl ,disp, hp, drat.

Проверить, что в наборе данных нет линейной зависимости (построить
зависимости между переменными, указанными в варианте, и проверить, что R^2 в
каждой из них не высокий). В случае, если R^2 большой, один из таких столбцов
можно исключить из рассмотрения.
Проверим линейную регрессию disp ~ cyl, hp, drat. R^2 в этой модели около 82%, поэтому
мы заключаем, что параметр disp зависит от других регрессоров линейно и может быть
удален и рассмотрения при построении математических моделей.

Значения коэффициентов в этой модели и их стандартные ошибки представлены в
таблице 1.

Зависимость cyl ~ disp, hp, drat. R^2 = 85%. Значение коэффициента детерминации больше
82% предыдущей зависимости, поэтому заключаем, что линейная зависимость есть.
Нежелательно использовать Cyl в линейных моделях. Значения коэффициентов в этой
модели и их стандартные ошибки представлены в таблице 2.

В регрессии hp~cyl, disp, drat значение R^2 близко к 72%. Параметр hp можно
использовать в линейной регрессии вместе с cyl, disp, drat. Значения коэффициентов в
этой модели и их стандартные ошибки представлены в таблице 3.

Проверим линейную регрессию drat ~ disp, hp, cyl. R^2 в этой модели около 55%, поэтому
мы заключаем, что параметр drat независит от других регрессоров линейно и может
быть использован при построении математических моделей. Значения коэффициентов в
этой модели и их стандартные ошибки представлены в таблице 4.

Таким образом, заключаем, что самая большая линейная рависимость у регрессора cyl
(порядка 85%), поэтому удаляем его из дальнейшего рассмотренияь, а остальные
указанные в задании регрессоры использовать при построении моделей линейной
регрессии можно.

Таблица 2 .1. Характеристики модели зависимости параметра disp от параметров cyl, drat,
hp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимост
и
(Intercept) 105.8694 137.1689 0.772 0.
cyl 39.3327 дек- 13 3.119 0.00418 **
hp 0.4039 0.2625 1.539 0.
drat - 49.4274 26.1188 - 1.892 0..
Таблица 2 .2. Характеристики модели зависимости параметра cyl от параметров disp, drat,
hp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимости
(Intercept) 5.701261 1.428829 3.990 0.000431 ***
disp 0.006555 0.002102 3.119` 0.004178 **
hp 0.009900 0.002993 3.308 0.002589 **
drat - 0.689084 0.333579 - 2.066 0.048220 *
Таблица 2.3. Характеристики модели зависимости параметра hp от параметров cyl, drat,
disp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимости
(Intercept) - 219.3802 86.3820 - 2.540 0.01693 *
cyl 28.3807 8.5802^ 3.308 0.00259 **
disp 0.1930 0.1254 1.539 0.
drat 40.5768 17.5732 2.309 0.02854 *
Таблица 2.4. Характеристики модели зависимости параметра drat от параметров cyl, disp,
hp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимости
(Intercept) 4.735125 0.301898 15.684 2.13e- 15 ***
disp - 0.002294 0.001212 - 1.892 0..
hp 0.003942 0.001707 2.309 0.0285 *
cyl - 0.191917 0.092905 - 2.066 0.0482 *
Построить линейную модель зависимой переменной от указанных в варианте
регрессоров по методу наименьших квадратов (команда lm пакета lmtest в языке
R). Оценить, насколько хороша модель, согласно: 1) R^2 , 2) p-значениям каждого
коэффициента.
Проверим линейную регрессию mpg ~ drat , disp, hp. Характеристики модели
зависимости mpg от регрессоров disp, drat, hp приведены в таблице 5.

Таблица 2 .5. Характеристики модели зависимости параметра mpg от параметров drat,
disp, hp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимости
(Intercept) 19.344293 6.370882 3.0360 0.00513 **
disp - 0.019232 0.009371 - 2.052 0.04960 *
hp - 0.031229 0.013345 - 2.340 0.02663 *
drat 2.714975 1.487366 1.825 0..
У данной модели R^2=75% , можно сделать вывод что она очень хороша. Disp,hp близки
к 0.05 по p-статистке (mpg в основном описывается через них) и также они зависят от
mpg отрицательно, у drat одна *, значит он менее сильно влияет на mpg и также он
зависит положительно от mpg.

Ввести в модель логарифмы регрессоров (если возможно). Сравнить модели и
выбрать наилучшую.
При решении этой задачи были проверены модели, в которых были добавлены
параметры: mpg~log(disp)+(hp)+(drat), mpg~log(disp)+log(hp)+(drat),
mpg~log(disp)+log(hp)+log(drat). Выбираем последнюю модели с логарифмами от всех
регрессоров так как,ее R^2=82% (>чем у всех предыдущих моделей)

Таблица 2.6. Характеристики модели зависимости параметра mpg от логарифмов
регрессоров drat, disp, hp в наборе данных Mtcars

Параметр \
Характеристики
Значение Std. Error t value Pr(>|t|) Уровень
значимости
(Intercept) 69.541 11.443 2,83e+04 1.49e- 06 ***
I(log(disp)) - 6.514 1,83e+04 - 3.177 0.0036 **
I(log(hp)) - 3.523 1.986 - 1.774 0..
I(log(drat)) 1.714 4.917 0.349 0.
Так как log(drat) в новой модели не влияет на mpg (нет звездочек и даже точки), не берем
его логарифм (здесь можно его вообще убрать).

mpg~log(disp)+log(hp) - наилучшая модель с R^2=82,9.

Код решения задачи и сведения о проверенных моделях приведены в Приложении 2.

Вводим в модель всевозможные попарные произведения регрессоров и их
квадраты.
При решении этой задачи были проверены модели, в которых были добавлены попарные
произведения регрессоров и их квадраты :

mpg~(disphp)+drat → R^2=71%, _mpg~(dispdrat)+hp → R^2=68%, mpg~(hpdrat)+disp →
R^2=72%, mpg~(disp^2)+(hp^2)+(drat^2),data) →R^2=69%, mpg~(disp^2hp^2)+(drat)
→R^2=66%, mpg~(disp^2hp^2)+(drat^2) →R^2=67%, mpg~(drat^2hp^2)+(disp^2)
→R^2=60%, mpg~(drat^2hp^2)+(disp) →R^2=70%, mpg~(drat^2disp^2)+(hp^2)
→R^2=56% , mpg~(drat^2*disp^2)+(hp) →R^2=62%,_

mpg~(log(disp)^2)+(log(hp)^2)+(log(drat)^2)+(log(hp)*log(drat)) →R^2=81%,

mpg~(log(disp))+(log(hp))+(log(drat))+(log(hp)*log(drat)) →R^2=82%,

mpg~(log(disp))+(log(hp))+(log(drat))+(log(disp)*log(hp)) →R^2=83,8%

Таким образом из 4ого пункта лучше всех по доле объясненного разброса в данных R^
= 83,8% : mpg~(log(disp))+(log(hp))+(log(drat))+(log(disp)*log(hp))

После выполнения первой части практической работы No2 можно сделать вывод что для
набора данных mtcars и четырех переменных mpg, disp, hp, drat применение
логарифмической функции к регрессорам может привести к линеаризации отношения

между переменными и улучшению качества модели, устранить гетероскедастичность
или облегчить интерпретацию коэффициентов.

Disp,hp близки к 0.05 по p-статистке (mpg в основном описывается через них) и также
они зависят от mpg отрицательно, у drat одна *, значит этот менее сильно влияет на mpg
и также он зависит положительно от mpg.

Использование логарифмов регрессоров в модели может иметь несколько причин,
которые могут приводить к улучшению показателя R^2:

1.Линейность отношения: В некоторых случаях зависимость между регрессорами и
зависимой переменной может быть нелинейной. Применение логарифмической
функции к регрессорам может привести к линеаризации отношения между
переменными и улучшению качества модели.

2.Устранение гетероскедастичности: Гетероскедастичность означает, что дисперсия
ошибок модели изменяется в зависимости от значений регрессоров. Если
гетероскедастичность присутствует в данных, логарифмирование регрессоров может
помочь устранить или уменьшить этот эффект, что может привести к более точной
модели и высокому значению R^2.

Интерпретация коэффициентов: Использование логарифмической шкалы для
регрессоров может упростить интерпретацию коэффициентов модели.
Логарифмическое преобразование может привести к изменению масштаба и
интерпретации коэффициентов как процентных изменений в зависимой переменной при
изменении регрессора на одну единицу.
Вторая часть практической работы No

Для зависимости, построенной при решении практического задания No2, оцените:

1,2) Доверительные интервалы для каждого коэффициента с p=95% и проверка
статистической гипотезы коэф = 0 :
Лучшая модель из первой части работы:
mpg~(log(disp))+(log(hp))+(log(drat))+(log(disp)*log(hp))

Число степеней свободы модели: 37-4=33, где 37 - число параметров набора данных
mtcars, 4 - количество линейно независимых переменных.

t-критериq Стьюдента = 2.034515.

Интервал для свободного коэффициента : [285.876751224019 , 58.2592487759815]

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале).

Интервал для log(disp): [-4.43748771814166 , -46.9385122818583]

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале).

Интервал для log(hp) : [-1.4508971835809 , -44.4931028164191]

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале).

Интервал для log(drat) : [8.41633865466867 , -12.3723386546687]

Этот коэффициент может быть равен 0, на уровне значимости 5% (нет 0 в этом
интервале).

Интервал для log((disp) * log(hp)) : [7.89682444674001, 0.334824446740012]

Этот коэффициент может быть равен 0, на уровне значимости 5% (нет 0 в этом
интервале).

Доверительный интервал для одного прогноза:
Таблица 1.7. Результаты предсказания лучшей модели.
Предсказание fit lwr upr
1 71.81007 43.79245 99.
Доверительный интервал для одного прогноза [43.79245, 99.8277]
После выполнения второй части работы No2 можно понять, что доверительные
интервалы модели в линейной регрессии являются статистическими инструментами,
которые позволяют оценить неопределенность и уверенность в оценках
коэффициентов регрессии. Они предоставляют информацию о диапазоне значений, в
котором, с определенной вероятностью, находится истинное значение параметра
модели.
На основе этих данных мы можем сделать следующие выводы:
Свободный коэффициент, log(disp) и log(hp) значимо отличаются от нуля. Это
означает, что эти переменные оказывают влияние на зависимую переменную (mpg) в
модели.
Коэффициент log(drat) не значимо отличается от нуля, что может говорить о его
незначительном влиянии на зависимую переменную.

Коэффициент log((disp) * log(hp)) также не значимо отличается от нуля, что может
указывать на его ограниченное влияние на модель.

На основе этого доверительного интервала мы можем сделать следующие выводы:

Предсказанное значение составляет 71.81007.
С вероятностью 95%, фактическое значение прогнозируемой переменной будет
находиться в интервале [43.79245, 99.8277].

Таким образом, мы можем быть уверены с 95% вероятностью, что фактическое
значение прогнозируемой переменной будет в указанном интервале. Это позволяет
нам оценить диапазон значений и измерить уровень неопределенности при
прогнозировании.

Отчет по задаче No
Задача 3

Постройте линейную регрессию зарплаты на все параметры, которые Вы выделили
из данных мониторинга. Не забудьте оценить коэффициент вздутия дисперсии VIF.
Построение вспомогательной регрессии salary~age + sex + higher_educ + status2 + dur

wed1 + wed2 + wed3. Переменные все хорошие , vif < 5
Поэкспериментируйте с функциями вещественных параметров: используйте
логарифмы, степени (хотя бы от 0.1 до 2 с шагом 0.1), произведения вещественных
регрессоров
Модели с логарифмами , степенями и произведениями

salary~(log(age+3)) + sex + higher_educ + status2 + dur+ wed1 + wed2 + wed3)
adj R^2 =0.025, параметры I(log(age+3))+ higher_educ + dur+wed1+wed2 значимые

salary~(log(age+3)) + sex + higher_educ + status2 +dur+ wed1 + wed2 + wed3)
adj R^2 =0.0256 , параметры I(log(age+3))+ higher_educ + dur+wed1+wed2 значимые

salary~(log(age+3)) + sex + higher_educ + status2 + dur+(agedur)+ wed1 + wed2 +
wed3), adj R^2 =0.02602 , параметры параметры I(log(age+3))+ higher_educ +
dur+wed1+wed2+I(agedur) значимые

salary~(log(age+3)) + sex + higher_educ + status2 + I(dur^2)+I(age*dur)+ wed1 + wed

wed3) , adj R^2 =0,02602 , параметры age + higher_educ + I(dur^2)+I(age*dur)+ wed
wed2 значимые
salary~I((age+3)^1.5) + sex + higher_educ + status2 + (dur^2)+I(age*dur)+ wed1 + wed

wed3), adj R^2 =0.0258 , параметры I(log(age+3))+higher_educ +wed1+wed2+
(agedur)+(dur^2)- значимые ; менее значимые (agedur)+wed3)
salary~(log(age+3)^(0.1)) + sex + higher_educ + status2 +(age*dur) +(dur)+ wed1 + wed

wed3), adj R^2 =0.02601, параметры (log(age+3))+higher_educ +wed1+wed2+ (dur)-
значимые ; менее значимые : (age*dur)
salary~I((age+3)^(0.1)) + sex + higher_educ + status2 +I((dur+1)^(0.9))+I(age*dur)+
wed1 + wed2 + wed3); adj R^2 =0.02602 , параметры I(log(age+3)^(0.1))+higher_educ

+wed1+wed2+ I((dur+1)^(0.9))- значимые ; менее значимые : I(age*dur)

salary+3)^2~(age+3^(1.9)) + sex + higher_educ + status2 +((dur+1)^(0.1))+(agedur)+
wed1 + wed2 + wed3), adj R^2 =0.025 , параметры (log(age+3)^(1.9))+higher_educ
+wed1+wed2+ ((dur+1)^(0.1))- значимые ; менее значимые : I(agedur)

(salary+3)^(0.1)~((age+3)^(0.1)) + sex + higher_educ + status2 +((dur+1)^(0.1))
+(agedur) + wed1+wed2+wed3), adj R^2 =0.02603=2,603% , параметры
I(log(age+3)^(0.1))+higher_educ +I((dur+1)^(0.1))+wed1+wed2 - значимые ; менее
значимые : I(agedur)

Выделите наилучшие модели из построенных: по значимости параметров,
включенных в зависимости, и по объяснённому с помощью построенных
зависимостей разбросу adjusted R^2 - R^2 adj.
Лучшая модель - (salary+3)^(0.1)~((age+3)^(0.1)) + sex + higher_educ + status
+((dur+1)^(0.1)) +(agedur) + wed1+wed2+wed3) , adj R^2 =0.02603=2,603% ,
параметры I(log(age+3)^(0.1))+higher_educ +I((dur+1)^(0.1))+wed1+wed2 значимые;
менее значимые: I(agedur)

Сделайте вывод о том, какие индивиды получают наибольшую зарплату.
Из последней (лучшей) модели можно сделать вывод что лучше всех зарабатывают
более взрослые респонденты с высшим образовнием, более долгой рабочей неделей,
(женатые или разведеные).

Оцените лучшие модели для подмножества индивидов, с высшим образованием, не
из города; женщины, с высшим образованием. Сделайте вывод о том, какие индивиды
получают наибольшую зарплату.
Подмножество : С высшим образованием, не из города

(salary+3)^(0.1)~I((age+3)^(0.1)) + sex + 1 + status2 +I((dur+1)^(0.1)) +I(age*dur)+

wed1 +wed2 +wed3)
Большой VIF у wed1 , потому здесь его можно убрать и на результат он не повлияет.

adj R^2 = 0.06632=6,632% , параметры I((age+3)^(0.1))+I((dur+1)^(0.1))+I(age*dur)

значимые

Вывод : В подмножестве( С высшим образованием, не из города) хорошо
зарабатывают более взрослые респонденты с более длинной рабочей неделей.

Подмножество : женщины с высшим образованием

(salary+3)^(0.1)~I((age+3)^(0.1)) + 0 + 1 + status2+I((dur+1)^(0.1))+I(age*dur)
+wed1+wed2+wed3)
Большой VIF у wed1 , потому здесь его можно убрать и на результат он не повлияет

adj R^2 = 0.07919=7,919% , параметры I((age+3)^(0.1))+I((dur+1)^(0.1))+I(age*dur)
значимые

Вывод: В подмножестве ( женщины с высшим образованием) хорошо зарабатывают
более взрослые респонденты с более длинной рабочей неделей.

На основании лучшей модели и анализа всей выборки можно сделать вывод, что
уровень заработной платы наиболее высок у респондентов с более старшим возрастом,
имеющих высшее образование, работающих больше часов в неделю и состоящих в
браке или состоявших в разводе.

Код решения задачи и сведения о проверенных моделях приведены в Приложении 3.

Отчет по задаче No
Задача 4

Обработайте набор данных набор данных, Video game sales
https://www.kaggle.com/gregorut/videogamesales, подготовив его к решению задачи
классификации.
Рисунок 4.1. Необработанный набор данных videogamesales.

Выделите целевой признак, указанный в последнем столбце таблицы, и удалите его из
данных, на основе которых будет обучаться классификатор:

Целевой признак - data['Genre']. Удаляем его из рассмотрения

Разделите набор данных на тестовую и обучающую выборку:

Y = data.loc[:, data.columns.isin(['Genre'])]
X = data.loc[:, data.columns.isin(['Year','Publisher','NA_Sales','EU_Sales', 'JP_Sales',
'Other_Sales'])]
X_train = X.sample(frac= 0.8 ,random_state= 0)
X_test= X.drop(X_train. index )
y_train = Y.sample(frac= 0.8 ,random_state= 0)
y_test= Y.drop(y_train. index )
importances = tree1.feature_importances_
array([0.20350082, 0.3018054 , 0.19686459, 0.13312733, 0.09516995,
0.0695319 ])
Вставка с кодом No1.

Важность признаков (делаем вывод что самым важным является : Publisher)

Постройте классификатор типа, DecisionTreeClassifier (решающее дерево), для задачи
классификации по параметру, указанному в последнем столбце. Оцените точность
построенного классификатора с помощью метрик precision, recall и F1 на тестовой
выборке.

Рисунок 4.2_. DecisionTreeClassifier построенный на_ данных videogamesales.

Опишем характеристики некоторых объектов класса 1 : Publisher >= 276, JP_Sales >=
0.005, Year>=2000.5

Опишем характеристики некоторых объектов класса 0: 137<=Publisher <=276 ,
JP_Sales <= 0.005, EU_Sales<=0.505.

DecisionTreeClassifier:

Accuracy : 0.8136893799877225

Precision : 0.6381022210221527

Recall : 0.642025980010945

F1-score : 0.6400029489960215

Так как данные не сбалансированы , делаем вывод что процент правильности
решающего дерева по метрике F1 порядка 64%.

Постройте классификатор типа Случайный Лес (Random Forest) для решения той же
задачи классификации. Оцените его качество с помощью метрик precision, recall и F1
на тестовой выборке.
Random Forest:

Accuracy : 0.875023247163846

Precision : 0.7641319847 186843

Recall : 0.6210986571366688

F1-score : 0.6535865952980463

Точность предсказания выросла и стала порядка 65%. Наши данные не
сбалансированы ,поэтому у алгоритма Random Forest, построенного на
несбалансированных данных, точность может быть ниже 70% по нескольким
причинам:

Предсказания деревьев: Random Forest состоит из множества деревьев, каждое из
которых строится на подвыборках данных. Если данные несбалансированы, то
некоторые деревья могут быть сильно смещены к предсказанию доминирующего
класса, игнорируя редкий класс. Это может привести к снижению точности для
редкого класса.
Случайные подвыборки: Random Forest случайным образом выбирает
подмножество данных для построения каждого дерева. Если редкий класс имеет
меньшее количество примеров, то существует вероятность, что случайная подвыборка
не будет содержать достаточное количество примеров этого класса. Это может
привести к недостаточной информации для корректного предсказания редкого класса.
Взвешивание классов: Random Forest позволяет задать веса классов, чтобы учесть
несбалансированность данных. Однако, если веса классов не были правильно
настроены или не соответствуют действительности, то это может повлиять на
точность предсказания редкого класса.
Ошибки деревьев: Деревья в Random Forest могут допускать ошибки
классификации, особенно для редкого класса. Если большинство деревьев в ансамбле
делает неправильное предсказание для редкого класса, то это может снизить точность
алгоритма в целом.
Чтобы улучшить точность Random Forest на несбалансированных данных, можно
применить следующие подходы:

Балансировка классов: Применить стратегии балансировки классов, такие как
upsampling или downsampling, чтобы сделать распределение классов более
равномерным. Это поможет увеличить представительность редкого класса и улучшить
точность его предсказания.
Настройка весов классов: Использовать веса классов для учета
несбалансированности данных. Корректное настройка весов классов может помочь
модели более справедливо учесть редкий класс и повысить точность его предсказания.
Использование подходов обработки несбалансированных данных: Применить
специальные методы, такие как SMOTE (Synthetic Minority Over-sampling
С помощью GridSearch переберите различные комбинации гиперпараметров: на
первой итерации задайте большие шаги (50 или 100) по числу деревьев n_estimators.
На следующих итерациях определите лучшее количество деревьев n_estimators с
точностью до 10.

Лучшие гиперпараметры : {'max_depth': 12, 'n_estimators': 110}
F1-score : 0.596405386446071 ~ 60%

Из проделанной практики делаем вывод что Случайный Лес + GridSearch дают
худшую точность = 60% > 64% = точность Решаюшего Дерева по метрике F1 , так как
данные не сбалансированы. Лучшую точность дает Random Forest без перебора
гиперпараметров (точность порядка 65%)

Код решения задачи и сведения о проверенных моделях приведены в Приложении 4.

Отчет по задаче No5
Задача 5

Задание 5. Предобработка данных и PCA Необходимо провести анализ датасета (
https://www.kaggle.com/datasets/tejashvi14/travel-insurance-prediction-data ) и сделать
обработку данных по предложенному алгоритму. Код подготовить в виде файлов .ipynb
и сделать отчет в виде ноутбука с описанием процесса анализа (.pdf).

Всего в наборе 1987 объектов и 9 признака.
Age- Возраст учащегося.

Employment Type - Тип занятости.

GraduateOrNot - Выпускник Или Нет.

AnnualIncome - Годовой доход.

FamilyMembers - Члены семьи.

ChronicDiseases - Хронические Заболевания.

FrequentFlyer - Часто летающий пассажир.

EverTravelledAbroad - Когда-либо путешествовал за границу.

TravelInsurance - Страхование путешествий.

Сколько категориальных признаков, какие?
В данном наборе данных 4 категориальных признака.К категориальным признакам
относятся:

Employment Type - Тип занятости.

GraduateOrNot - Выпускник Или Нет.

FrequentFlyer - Часто летающий пассажир.

EverTravelledAbroad - Когда-либо путешествовал за границу.

Столбец с максимальным количеством уникальных значений категориального
признака?
Все эти 4 столбца имеют максимальное количество уникальных значений

Есть ли бинарные признаки?
Да, к таким признакам относятся:

Employment Type- Тип занятости.

GraduateOrNot - Выпускник Или Нет.

FrequentFlyer - Часто летающий пассажир.

EverTravelledAbroad - Когда-либо путешествовал за границу.

ChronicDiseases - Хронические Заболевания.

TravelInsurance - Страхование путешествий.

Какие числовые признаки?
К числовым признакам относятся:

Age - Возраст учащегося.

AnnualIncome - Годовой доход.

FamilyMembers - Члены семьи.

Есть ли пропуски?
Пропусков нет.

Сколько объектов с пропусками?
0

Столбец с максимальным количеством пропусков?
Такого столбца нет, так как пропусков в наборе данных нет.

Есть ли на ваш взгляд выбросы, аномальные значения?
Здесь нет сильных различий между максимальными значениями и средними, потому
можно сделать вывод что вбросов/аномалий в данных нет.

Столбец с максимальным средним значением после нормировки признаков через
стандартное отклонение?
В ходе нормализации будем нормализовывать числовые признаки, но которые
принимают реальные вещественные значения, а не являются описанием какого-либо
признака.

К таким признакам относятся:

Age

AnnualIncome

Столбец с целевым признаком?
В наборе данных можно выделить один целевой признак: TravelInsurance - Страхование
путешествий.

Сколько объектов попадает в тренировочную выборку при использовании
train_test_split с параметрами test_size = 0.3, random_state = 42?
При заданных параметрах в тренировочную выборку попадает 597 объектов.

Между какими признаками наблюдается линейная зависимость (корреляция)?
Из таблицы можно сделать вывод, что в этом датасете нет линейно-зависимых
параметров.

Сколько признаков достаточно для объяснения 90% дисперсии после применения
метода PCA?
Для объяснения 90% дисперсии после применения метода PCA достаточно 10
компонент.

Рисунок 5.1. График важности компонент PCA и увеличения процента объясненной
дисперсии.

Какой признак вносит наибольший вклад в первую компоненту?
Признак $AnnualIncome_std$ вносит наибольший вклад в первую компоненту.

Построить двухмерное представление данных с помощью алгоритма t-SNE. На
сколько кластеров визуально, на ваш взгляд, разделяется выборка? Объяснить смысл
кластеров
Рисунок 5.2. График визуального представления t-SNE.

Выборка раделяется на 2 класса, верхний кластер показывает что объекты покупают
страховку, а объекты нижнего кластера - не покупают.

Код решения задачи и сведения о проверенных моделях приведены в Приложении 5.

Отчет по задаче No6
Задача 6

Для набора данных (https://www.kaggle.com/datasets/tejashvi14/travel-insurance-
prediction-data ) построить модель предсказания: будет ли клиент брать страховку,
включающую покрытие рисков, связанных с коронавирусом? Оценить вклад каждой
компоненты. Есть ли выбросы среди данных, какие? Применить алгоритмы
кластеризации. Охарактеризовать кластеры.

Для описания 90% дисперсии достаточно 10 компонент PCA.

AgglomerativeClustering (Иерархической кластеризация)

Рисунок 6.1. Дендрограмма иерархического алгоритма кластеризации.

Дендрограмма иерархического алгоритма кластеризации дает нам следующее:

иерархическую структуру кластеров, информацию о расстояниях между кластерами
помощь в определении оптимального числа кластеров.

По этому графику можно сделать выводы , что данные оптимально разбиваются на 5
кластеров на высоте 40.

Визуальное представление кластеров:

Рисунок 6.2. График разбиения данных на 2 кластера.

Рисунок 6.3. График разбиения данных на 3 кластера.

Рисунок 6.4. График разбиения данных на 4 кластера.

Можно сделать вывод , что чем больше кластеров давать алгоритму, тем менее
разделимы они получаются на наших данных.

DBSCAN
Рисунок 6.5. График результатов работы алгоритма DBSCAN.

Индивиды разделились на 2 класса, зеленая группа покупает страховки, а синяя группа

нет.
K-means

Рисунок 6.6. График зависимости суммы квадратов ошибок от количества кластеров.

Из рисунка 6 видно, что оптимальное количество кластеров - 4.

Рисунок 6.7. График результатов работы алгоритма k-means.

Вычисленные центры классов:

[[-0.02338718 0.01653908 0.12747969 -0.38446642 1.40057273 0.80884714]

[ 0.03223941 -0.00598203 -0.97650456 1.29426444 0.19268617 -0.70149935]

[-0.00717411 0.00362513 -0.33309664 -0.52959815 -0.73477452 0.18495898]
[ 0.00469648 -0.01965695 1.65835384 0.04514494 -0.32784704 -0.50751104]]
Объясним смысл каждого из 4рех кластеров:

Таблица 6.1. Таблица объясняющая усредненные характеристики каждого класса

Age Employment Type
GraduateOr
Not
AnnualInco
me
ChronicDisea
ses
0 0.007864 0.629113 0.912942 - 0.001067 0.972436
1 - 0.011303 0.685189 0.889088 0.055563 0.271174
2 0.006592 0.756882 0.796469 - 0.043795 0.030614
3 - 0.009633 0.752306 0.850673 0.027383 - 0.008222
FrequentFl
yer
EverTravelledA
broad
TravelInsura
nce FamMem_2
FamMem_
3
0.071783 0.210861 0.403489 0.052937 0.236327
0.309669 0.158524 0.233949 0.050433 - 0.110043
0.239356 0.219074 0.437767 0.047907 0.458790
0.193985 0.149017 0.280502 0.033298 - 0.072420
FamMem_
4 FamMem_5 FamMem_6 FamMem_7
FamMem_
8 FamMem_9
0.316191 0.017050 0.197313 0.117420 0.021395 0.041367
- 0.077586 0.891660 0.115490 0.078440 0.027740 0.023867
0.064806 0.078950 0.185096 0.099687 0.037007 0.027757
0.952398 - 0.057424 0.052043 0.049508 0.026487 0.016110
Люди среднего возраста, которые работают в частном секторе, учились в
колледже, имеют чуть ниже среднего заработок, количество детей - среднее. У
них есть хроническое заболевание. Покупают авиаилеты и путешествуют за
рубеж редко, кто-то покупал страховой пакет, а кто-то нет
Люди среднего возраста, которые работают в частном секторе, учились в
колледже, имеют средний заработок , количество детей - среднее. У них нет
болезней. Покупают авиаилеты и путешествуют за рубеж редко, не покупают
страховой пакет.
Люди немного старше среднего, которые работают в государственном секторе,
учились в колледже, имеют заработок ниже среднего, количество детей - среднее.
У них нет болезней. Покупают авиаилеты и путешествуют за рубеж редко,
большинство не купило страховой пакет.
Люди среднего возраста, которые работают в частном секторе, учились в
колледже, имеют заработок выще среднего, количество детей - среднее. У них
нет болезней. Покупают авиаилеты и путешествуют за рубеж чаще обычного
(больше половины да чем нет), почти все купили страховой пакет.
Классификация на основе кластеров лучшего алгоритма (K-means)

Строим дерево решений на наших кластерах:

Перебираем высоту дерева:

2: 0.7674418604651163

3: 0.7751937984496124

4: 0.7906976744186046

5: 0.7984496124031008

6: 0.7984496124031008

7: 0.8062015503875969

8: 0.7829457364341085

9: 0.7441860465116279

Лучший результат дает дерево с глубиной 4.

Строим дерево решений глубиной 4. Получили модель с правильностью порядка 76% по
метрике F1.

Важность признаков:
[0.1382899 0.02543763 0.00905349 0.54925611 0. 0.01466946
0.06357554 0. 0. 0. 0. 0.08450275
0.11521512 0. 0. ]

Самым важным признаком оказался - Annuallncome.

Опишем характеристики некоторых объектов класса 1. AnnualIncome >= 1.041,
GraduateOrNot >= 0.5, Age в окрестности - 0.052, EverTravelledAdroab >= 0.5.

Опишем характеристики некоторых объектов класса 0. AnnualIncome <= 1.041,
Age<=0.978 or Age >=0.978,
EverTravelledAdroab <= 0.5, FamMem_6 <= 0.5.

Рисунок 6.8. DecisionTreeClassifier построенный на кластерах алгоритма k-means.

Попробуем RandomForestClassifier + GridSearch

Лучшие гиперпараметры:{'max_depth': 9, 'n_estimators': 250}

Best score F1 is: 0.7005622871624296

Лучшие гиперпараметры:{'max_depth': 9, 'n_estimators': 230}

Таблица 6.2. Оценочные метрики RandomForestClassifier + GridSearch.

precision recall f1-score support
0 0.77 0.94 0.84 78
1 0.85 0.57 0.68 51
accuracy 0.79 129
macro avg 0.81 0.75 0.76 129
weighted avg 0.80 0.79 0.78 129
F1-score : 0.7631417885073104
Лучший F1 = 76%. Это также меньше у дерева решений.

Попробуем GradientBoostingClassifier:

Таблица 6.3. Оценочные метрики GradientBoostingClassifier.

precision recall f1-score support
0 0.76 0.92 0.83 78
1 0.82 0.5 0.66 51
accuracy 0.78 129
macro avg 0.79 0.74 0.75 129
weighted avg 0.78 0.78 0.76 129
F1-score : 0.7455967358041483

Получили F1 = 74%, все еще меньше чем у дерева решений (F1=76%), оставляем дерево
решений в качестве лучшей модели.

Таким образом, можно сделать вывод, что страховку с большей вероятностью купят
люди среднего возраста, которые работают в частном секторе, учились в колледже,
имеют заработок выше среднего, количество детей - среднее. У них нет болезней. Они
покупают авиабилеты и путешествуют за рубеж чаще обычного ( больше половины да
чем нет).

Заключение
Практика No1:
Коэффициент детерминации (Coefficient of Determination, R^2): R^2 измеряет
пропорцию дисперсии зависимой переменной, объясненную моделью. Значение
R^2 находится в диапазоне от 0 до 1, где 1 означает, что модель полностью
объясняет вариацию данных, а 0 - что модель не объясняет никакую вариацию.
Чем ближе R^2 к 1, тем лучше модель.
Зависимость Examination~Fertility. R2 = 40%. Значение коэффициента
детерминации меньше 80%, поэтому заключаем, что линейной зависимости нет.
p-value = 9.45*(10**-7) , *** у Fertility означают сильную взаимосвязь Fertility и
Examination. Fertility и Examination связаны отрицательно (-0.41)
Зависимость Examination~Education. R2 = 47%. Значение коэффициента
детерминации меньше 80%, поэтому заключаем, что линейной зависимости нет.
p-value = 4.811*(10**-8) , *** у Education означают сильную взаимосвязь
Education и Examination. Education и Examination связаны положительно (0.57)
Также из проделанной практики можно сделать вывод о том, что Examination не
зависит линейно от Education и Fertility.
Практика No2:
Целевая переменная mpg описывается через log(disp) , log(hp) на 82,9%
После выполнения первой части практической работы No2 можно сделать вывод
что для набора данных mtcars и четырех переменных mpg, disp, hp, drat
применение логарифмической функции к регрессорам может привести к
линеаризации отношения между переменными и улучшению качества модели,
устранить гетероскедастичность или облегчить интерпретацию коэффициентов.
После выполнения второй части практической работы No2 можно понять что
доверительные интервалы модели предоставляют информацию о статистической
значимости, уровне неопределенности и помогают сделать более
информированные выводы о модели и ее прогностической силе.
Практическая работа No3:
Лучшая модель - (salary+3)^(0.1)~((age+3)^(0.1)) + sex + higher_educ + status2
+((dur+1)^(0.1)) +(age*dur) + wed1+wed2+wed3) , adj R^2 =0.02603=2,603% ,
параметры I(log(age+3)^(0.1))+higher_educ +I((dur+1)^(0.1))+wed1+wed2
значимые; менее значимые: I(age*dur).
Из последней (лучшей) модели можно сделать вывод что лучше всех
зарабатывают более взрослые респонденты с высшим образовнием, более долгой
рабочей неделей, (женатые или разведеные).
В подмножестве ( С высшим образованием, не из города) хорошо зарабатывают
более взрослые респонденты с более длинной рабочей неделей.
В подмножестве ( женщины с высшим образованием) хорошо зарабатывают
более взрослые респонденты с более длинной рабочей неделей.
Практическая работа No4:
Из проделанной практики делаем вывод что по метрике F1: Случайный Лес +
GridSearch дают худшую точность = 60% > 64% = точность Решаюшего Дерева
по метрике F1 , так как данные не сбалансированы. Лучшую точность дает
Random Forest без перебора гиперпараметров (точность порядка 65%)
Улучшить алгоритмы классификации можно сбалансировав данные (убрать
данные большего класса или сгенерировать приближенные данные другого
класса.
Практическая работа No5:
В наборе данных содержится информация о 1987 объектах и 9 признаках.
Из них 4 признака являются категориальными: Employment Type,
GraduateOrNot, FrequentFlyer и EverTravelledAbroad.
Все категориальные признаки имеют максимальное количество уникальных
значений.
Все признаки, включая категориальные, могут быть рассмотрены как
бинарные, так как они принимают только два возможных значения.
Числовыми признаками являются: Age, AnnualIncome и FamilyMembers.
В наборе данных отсутствуют пропуски.
Количество объектов с пропусками составляет 0.
В данных нет выбросов или аномальных значений, так как максимальные
значения не сильно отличаются от средних значений.
После нормировки признаков через стандартное отклонение, столбец с
максимальным средним значением будет зависеть от конкретных значений в
данных и не может быть определен без их рассмотрения.
Целевым признаком является TravelInsurance - страхование путешествий.
При использовании train_test_split с параметрами test_size = 0.3 и random_state
= 42, в тренировочную выборку попадает 597 объектов.
В данном наборе данных нет линейной зависимости (корреляции) между
признаками.
Для объяснения 90% дисперсии после применения метода PCA достаточно 10
компонент.
Признак AnnualIncome_std вносит наибольший вклад в первую компоненту
при применении PCA.
По визуальному представлению данных с помощью алгоритма t-SNE выборка
разделяется на 2 кластера, где верхний кластер соответствует объектам, которые
покупают страховку, а нижний кластер - объектам, которые не покупают
страховку.
Практическая работа No6:
Таким образом, можно сделать вывод, что люди среднего возраста, работающие
в частном секторе, закончившие колледж, выше среднего уровня дохода, среднее
количество детей и без каких-либо заболеваний, с большей вероятностью
приобретут страховку. Кроме того, эта группа людей чаще, чем обычно (более
половины), покупает авиабилеты и осуществляет поездки за рубеж.
Дерево решений, основанное на кластеризации с использованием алгоритма k-
means, проявляет точность около 76% по метрике F1. Это означает, что модель
достигает высокого уровня правильных предсказаний, учитывая общую точность
и полноту модели. Метрика F1 является сбалансированным показателем,
учитывающим как точность, так и полноту, что делает ее полезной для оценки
производительности классификационных моделей. Общая точность дерева
решений на основе k-means-кластеризации находится на уровне 76%, что
указывает на способность модели классифицировать данные с неплохой
точностью и показывать удовлетворительные результаты.
Список литературы
Introduction to Econometrics with R/C. Hanck, M. Arnold, A. Gerber, M. Schmelzer.
Essen, Germany: University of Duisburg-Essen, 2021.
Айвазян, С.А. Основы эконометрики/С.А. Айвазян, В.С. Мхитарян – Москва:
Изд. объединение «ЮНИТИ», 1998. – 1005 с.
Вербик, М. Путеводитель по современной эконометрике/М. Вербик – Москва:
«Научная книга», 2008. – 616 с.
Доугерти, К. Введение в эконометрику/К. Доугерти – Москва: ИНФРА-М, 2009.
465 с.
Магнус, Я.Р. Эконометрика. Начальный курс/Я.Р. Магнус, П.К. Катышев, А.А.
Пересецкий – Москва: Изд-во «ДЕЛО», 2004. – 576 с.
Приложения
Приложение 1.

library("lmtest")
data = swiss
#1)
#среднее арифметическое,Дисперсия и СКО Examination
mean(data$Examination)
var(data$Examination)
sd(data$Examination)
#среднее арифметическое,Дисперсия и СКО Fertility
mean(data$Fertility)
var(data$Fertility)
sd(data$Fertility)
#среднее арифметическое,Дисперсия и СКО Education
mean(data$Education)
var(data$Education)
sd(data$Education)
#2,3,4)
#Регрессия y =ax*b для Examination~Fertility
model1=lm(Examination~Fertility,data)
model1
summary(model1)
# Оценка по R^2 : модель среднего качества, выдает правильные ответы в 40%
случаев
# p-value = 9.45*(10**-7) , *** у Fertility означают сильную взаимосвязь Fertility и
Examination
#Fertility и Examination связаны отрицательно (-0.41)
#Регрессия y =ax*b для Examination~Education
model2=lm(Examination~Education,data)
model2
summary(model2)
# Оценка по R^2 : модель среднего качества, выдает правильные ответы в 47%
случаев
# p-value = 4.811*(10**-8) , *** у Education означают сильную взаимосвязь
Education и Examination
# Education и Examination связаны положительно (0.57)
Рисунок 1. Результат работы встроенных функций подсчета среднего
арифметического,дисперсии и СКО для Examination,Education, Fertility.

Приложение 2.

library(car)
library("lmtest")
# Вариант 7
data = na.omit(mtcars)
help(mtcars)
# Первая часть практической
"Прочитав информацию из набора данных, выполните задачи:
Проверьте, что в наборе данных нет линейной зависимости (построить
зависимости
между переменными, указанными в варианте, и проверить, что R^2 в каждой из них
невысокий). В случае, если R^2 большой, один из таких столбцов можно исключить
из рассмотрения.
Постройте линейную модель зависимой переменной от указанных в варианте
регрессоров по методу наименьших квадратов (команда lm пакета lmtest в языке R).
Оценить, насколько хороша модель, согласно: 1) R^2, 2) p-значениям каждого
коэффициента.
Введите в модель логарифмы регрессоров (если возможно). Сравнить модели и
выбрать наилучшую.
Введите в модель всевозможные произведения пар регрессоров, в том числе
квадраты регрессоров. Найдите одну или несколько наилучших моделей по доле
объясненного разброса в данных R^2."
#1 Исследуем линейную завимсимость регрессоров

model1=lm(mpg~cyl+disp+hp+drat,data)

vif(model1)#vif=7.7 самый большой у cyl

summary(lm(disp~cyl+hp+drat,data))#тоже зависим и vif=6.2 > 5

summary(lm(cyl~disp+hp+drat,data))#убеждаемся что больше всех линейно зависим
cyl

summary(lm(hp~cyl+disp+drat,data))

summary(lm(drat~disp+hp+cyl,data))

#удаляем cyl из исследования

#2 Строим парную регрессию с менее зависимыми между собой регрессорами

model2=lm(mpg~disp+hp+drat,data)

summary(model2) #1)R^2=75% модель очень хороша

#2)disp,hp близки к 0.05 по p-статистке (mpg в основном описывается через них)

#и также они зависят от mpg отрицательно, у drat одна *, значит он менее сильно
влияет на mpg

и также он зависит положительно от mpg
#3 Вводим в модель логарифмы регрессоров

vif(lm(I(log(drat))~I(log(disp))+I(log(hp)),data))

vif(lm(I(log(disp))~I(log(drat))+I(log(hp)),data))

vif(lm(I(log(hp))~I(log(drat))+I(log(disp)),data))

#Из vif трех моделей видим что новые столбцы почти линейнонезависимы
междусобой

summary(lm(mpg~I(log(disp))+I((hp))+I((drat)),data))#R^2=81%

summary(lm(mpg~I(log(disp))+I(log(hp))+I((drat)),data))#R^2=82%

model3=lm(mpg~I(log(disp))+I(log(hp))+I(log(drat)),data)

summary(model3)

#Выбираем новую модель так как,ее R^2=82% (>75% пердыдущей модели)

#Так как log(drat) в новой модели не влияет на mpg (нет звездочек и даже точки),

#не берем его логарифм (здесь можно его вообще убрать)

model4=lm(mpg~I(log(disp))+I(log(hp)),data)

summary(model4)#наилучшая модель R^2=82,9%

#4 Всевозможные пары регрессоров и их квадраты

model5=lm(mpg~I(disp*hp)+drat,data)#R^2=71%

summary(model5)

vif(model5)

model6=lm(mpg~I(disp*drat)+hp,data)#R^2=68%

summary(model6)

vif(model6)

model7=lm(mpg~I(hp*drat)+disp,data)#R^2=72%

summary(model7)

vif(model7)

model8=lm(mpg~I(disp^ 2 )+I(hp^ 2 )+I(drat^ 2 ),data)#R^2=69%

summary(model8)

vif(model8)

model9=lm(mpg~I(disp^ 2 *hp^ 2 )+I(drat),data)#R^2=66%

summary(model9)

vif(model9)

model10=lm(mpg~I(disp^ 2 *hp^ 2 )+I(drat^ 2 ),data)#R^2=67%

summary(model10)

vif(model10)

model11=lm(mpg~I(drat^ 2 *hp^ 2 )+I(disp^ 2 ),data)#R^2=60%

summary(model11)

vif(model11)

model12=lm(mpg~I(drat^ 2 *hp^ 2 )+I(disp),data)#R^2=70%

summary(model12)

vif(model12)

model13=lm(mpg~I(drat^ 2 *disp^ 2 )+I(hp^ 2 ),data)#R^2=56%

summary(model13)

vif(model13)

model14=lm(mpg~I(drat^ 2 *disp^ 2 )+I(hp),data)#R^2=62%

summary(model14)

vif(model14)

model15=lm(mpg~I(log(disp)^ 2 )+I(log(hp)^ 2 )+I(log(drat)^ 2 )+I(log(hp)*log(drat)),data)

summary(model15)#R^2=81%

model15=lm(mpg~I(log(disp))+I(log(hp))+I(log(drat))+I(log(hp)*log(drat)),data)

summary(model15)#R^2=82%

model16=lm(mpg~I(log(disp))+I(log(hp))+I(log(drat))+I(log(disp)*log(hp)),data)

summary(model16)#R^2=83,8%

#Таким образом из 4ого пункат лучше всех

по доле объясненного разброса в данных R^2 = 83,8%
model16=lm(mpg~I(log(disp))+I(log(hp))+I(log(drat))+I(log(disp)*log(hp)),data)

"______________________________________________________________________
____________"
#Вторая часть практической

Для зависимости, построенной при решении практического задания No 2 , оцените:

Доверительные интервалы для всех коэффициентов в модели, p = 95%.
Сделайте вывод о отвержении или невозможности отвергнуть статистическую
гипотезу о том, что коэффициент равен 0.

Доверительный интервал для одного прогноза (p = 95 %, набор значений
регрессоров выбираете сами)."

#1,2) Доверительные интервалы для каждого коэфицента с p= 95 % и проверка
статистической гипотезы коэф = 0

model16=lm(mpg~I(log(disp))+I(log(hp))+I(log(drat))+I(log(disp)*log(hp)),data)

summary(model16)#R^2=83,8%

#Число степеней свободы модели:37-4=33

crit=qt(0.975, 33 )# t-критериq Стьюдента = 2.034515

coef(model16)

count_interval <- function(qt,cof,st_er){

lw <- cof-qt*st_er

up <- cof+qt*st_er

newList <- list("lw" = lw, "up" = up)

return(newList)

}

interval=count_interval(crit,172.068,55.939)

sprintf("[%s , %s]",interval["up"],interval["lw"])#Интервал для свободного коэф

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале)

interval=count_interval(crit,-25.688,10.445)

sprintf("[%s , %s]",interval["up"],interval["lw"])#Интревал для log(disp)

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале)

interval=count_interval(crit,-22.972,10.578)

sprintf("[%s , %s]",interval["up"],interval["lw"])#Интревал для log(hp)

Отвергаем гипотезу о том, что этот коэффициент может быть равен 0, на уровне
значимости 5% (нет 0 в этом интервале)

interval=count_interval(crit,-1.978,5.109)

sprintf("[%s , %s]",interval["up"],interval["lw"])#Интревал для log(drat)

Этот коэффициент может быть равен 0, на уровне значимости 5% (нет 0 в этом
интервале)

interval=count_interval(crit,3.781,2.023)

sprintf("[%s , %s]",interval["up"],interval["lw"])#Интревал для log(disp) * log(hp))

Этот коэффициент может быть равен 0, на уровне значимости 5% (нет 0 в этом
интервале)

#3 Доверительный интервал для одного прогноза

new.data = data.frame(disp = 10 , hp = 15 , drat=3.5)

predict(model16, new.data, interval = "confidence")

#Доверительный интервал для одного прогноза [43.79245, 99.8277]
Приложение 3.

install.packages("devtools")
devtools::install_github("bdemeshev/rlms")
library("lmtest")
library("rlms")
library("dplyr")
library("GGally")
library("car")
library("sandwich")
data <- read.csv("Downloads/r26i_os26b.csv")
#glimpse(data)
data2 = select(data, vj13.2, v_age, vh5, v_educ, status, vj6.2, v_marst)
#исключаем строки с отсутствующими значениями NA
data2 = na.omit(data2)
#зарплата c элементами нормализации
data2$vj13.2
sal = as.numeric(data2$vj13.2)
sal1 = as.character(data2$vj13.2)
sal2 = lapply(sal1, as.integer)
sal = as.numeric(unlist(sal2))
mean(sal)
data2["salary"] = (sal - mean(sal)) / sqrt(var(sal))

data2["salary"]

#возраст c элементами нормализации

age1 = as.character(data2$v_age)

age2 = lapply(age1, as.integer)

age3 = as.numeric(unlist(age2))

data2["age"]= (age3 - mean(age3)) / sqrt(var(age3))

data2["age"]

#пол

data2["sex"]=data2$vh5

data2$sex[which(data2$sex!=' 1 ')] <- 0

data2$sex[which(data2$sex==' 1 ')] <- 1

data2$sex = as.numeric(data2$sex)

#образование

data2["h_educ"] = data2$v_educ

#data2["h_educ"] = lapply(data2$v_educ, as.character)

data2["higher_educ"] = data2$v_educ

data2["higher_educ"] = 0

data2$higher_educ[which(data2$h_educ==' 21 ')] <- 1

data2$higher_educ[which(data2$h_educ==' 22 ')] <- 1

data2$higher_educ[which(data2$h_educ==' 23 ')] <- 1

#населенный пункт

data2["status1"]=data2$status

#data2["status1"] = lapply(data2$status, as.character)

data2["status2"] = 0

data2$status2[which(data2$status1==' 1 ')] <- 1

data2$status2[which(data2$status1==' 2 ')] <- 1

data2$status2 = as.numeric(data2$status2)

#продолжительность рабочей недели

dur1 = as.character(data2$vj6.2)

dur2 = lapply(dur1, as.integer)

dur3 = as.numeric(unlist(dur2))

data2["dur"] = (dur3 - mean(dur3)) / sqrt(var(dur3))

#семейное положение

data2["wed"]= data2$v_marst

#data2["wed"] = lapply(data2$v_marst, as.character)

data2$wed1 = 0

data2$wed1[which(data2$wed==' 2 ')] <- 1

data2$wed1 = as.numeric(data2$wed1)

data2["wed2"] = lapply(data2["wed"], as.character)

data2$wed2 = 0

data2$wed2[which(data2$wed==' 4 ')] <- 1

data2$wed2[which(data2$wed==' 5 ')] <- 1

data2$wed2 = as.numeric(data2$wed2)

data2["wed3"]=data2$v_marst

data2$wed3 = 0

data2$wed3[which(data2$wed==' 1 ')] <- 1

data2$wed3 = as.numeric(data2$wed3)

data2 = na.omit(data2)

#учет инфляции: +3.4% к показателям зар.платы 2017 года, +2.52% к показателям
2016 года

data2["salary"]

data2["salary"] = data2["salary"]*1.034

data3 = select(data2, salary, age, sex, higher_educ, status2, dur, wed1,wed2,wed3)

glimpse(data3)

#Пункт 1 построение вспомогательной регресии salary~age + sex + higher_educ +
status2 + dur + wed1 + wed2 + wed3

model_salary = lm(data = data3, salary~age + sex + higher_educ + status2 + dur + wed1

wed2 + wed3)
summary(model_salary)

vif(model_salary) #переменные все хорошие , vif < 5

#Пункт 2 модели с логарифмами , степенями и произведениями

model2 = lm(data = data3, salary~I(log(age+ 3 )) + sex + higher_educ + status2 + dur+
wed1 + wed2 + wed3)

summary(model2)

vif(model2)

#adj R^2 =0.025, параметры I(log(age+3))+ higher_educ + dur+wed1+wed2 значимые

model3 = lm(data = data3, salary~I(log(age+ 3 )) + sex + higher_educ + status2 +dur+ wed1

wed2 + wed3)
summary(model3)

vif(model3)

#adj R^2 =0.0256 ,параметры I(log(age+3))+ higher_educ + dur+wed1+wed2 значимые

model5 = lm(data = data3, salary~I(log(age+ 3 )) + sex + higher_educ + status2 +
dur+I(age*dur)+ wed1 + wed2 + wed3)

summary(model5)

vif(model5)

#adj R^2 =0.02602 , параметры параметры I(log(age+3))+ higher_educ +
dur+wed1+wed2+I(age*dur) значимые

model6 = lm(data = data3, salary~I(log(age+ 3 )) + sex + higher_educ + status2 +
I(dur^ 2 )+I(age*dur)+ wed1 + wed2 + wed3)

summary(model6)

vif(model6)

#adj R^2 =0,02602 , параметры age + higher_educ + I(dur^2)+I(age*dur)+ wed1 + wed2
значимые

model7 = lm(data = data3, salary~I((age+ 3 )^1.5) + sex + higher_educ + status2 +
I(dur^ 2 )+I(age*dur)+ wed1 + wed2 + wed3)

summary(model7)

vif(model7)

#adj R^2 =0.0258 , параметры I(log(age+3))+higher_educ +wed1+wed2+ I(agedur)+
I(dur^2)- значимые; менее значимые I(agedur)+wed3)

model4 = lm(data = data3, salary~I(log(age+ 3 )^(0.1)) + sex + higher_educ + status2
+I(age*dur) +I(dur)+ wed1 + wed2 + wed3)

summary(model4)

vif(model4)

#adj R^2 =0.02601, параметры I(log(age+3))+higher_educ +wed1+wed2+ I(dur)-
значимые; менее значимые: I(age*dur)

model8 = lm(data = data3, salary~I((age+ 3 )^(0.1)) + sex + higher_educ + status2
+I((dur+ 1 )^(0.9))+I(age*dur)+ wed1 + wed2 + wed3)

summary(model8)

vif(model8)

#adj R^2 =0.02602 , параметры I(log(age+3)^(0.1))+higher_educ +wed1+wed2+
I((dur+1)^(0.9))- значимые; менее значимые: I(age*dur)

model9 = lm(data = data3, (salary+ 3 )^ 2 ~I(age+ 3 ^(1.9)) + sex + higher_educ + status2
+I((dur+ 1 )^(0.1))+I(age*dur)+ wed1 + wed2 + wed3)

summary(model9)

vif(model9)

#adj R^2 =0.025 ,параметры I(log(age+3)^(1.9))+higher_educ +wed1+wed2+
I((dur+1)^(0.1))- значимые; менее значимые: I(age*dur)

model10 = lm(data = data3, (salary+ 3 )^(0.1)~I((age+ 3 )^(0.1)) + sex + higher_educ +
status2+I((dur+ 1 )^(0.1))+I(age*dur)+wed1+wed2+wed3)

summary(model10)

vif(model10)

#adj R^2 =0.02603=2,603% , параметры I(log(age+3)^())+higher_educ
+I((dur+1)^(0.1))+wed1+wed2 - значимые; менее значимые: I(age*dur)

#Пункт 3

#Лучшая модель - model10 ; #adj R^2 =0.02603, параметры
I(log(age+3)^())+higher_educ +I((dur+1)^(0.1))+wed1+wed2 значимые; менее
значимые: I(age*dur)

#Пункт 4

##Из последней (лучшей model10) модели можно сделать вывод что лучше всех
зарабатывают более взрослые респонденты

с высшим образовнием, более долгой рабочей неделей, (женатые или разведеные)
#Пункт 5

#Вариант 7. Файл 17. Подмножество С высшим образованием, не из города;
женщины, с высшим образованием

data <- read.csv("Downloads/r17i_os26b.csv")

data2 = select(data, mj13.2, m_age, mh5, m_educ, status, mj6.2, m_marst)

#исключаем строки с отсутствующими значениями NA

data2 = na.omit(data2)

#зарплата c элементами нормализации

data2$mj13.2

sal = as.numeric(data2$mj13.2)

sal1 = as.character(data2$mj13.2)

sal2 = lapply(sal1, as.integer)

sal = as.numeric(unlist(sal2))

mean(sal)

data2["salary"] = (sal - mean(sal)) / sqrt(var(sal))

data2["salary"]

#возраст c элементами нормализации

age1 = as.character(data2$m_age)

age2 = lapply(age1, as.integer)

age3 = as.numeric(unlist(age2))

data2["age"]= (age3 - mean(age3)) / sqrt(var(age3))

data2["age"]

#пол

data2["sex"]=data2$mh5

data2$sex[which(data2$sex!=' 1 ')] <- 0

data2$sex[which(data2$sex==' 1 ')] <- 1

data2$sex = as.numeric(data2$sex)

#образование

data2["h_educ"] = data2$m_educ

data2["higher_educ"] = data2$m_educ

data2["higher_educ"] = 0

data2$higher_educ[which(data2$h_educ==' 21 ')] <- 1

data2$higher_educ[which(data2$h_educ==' 22 ')] <- 1

data2$higher_educ[which(data2$h_educ==' 23 ')] <- 1

#населенный пункт

data2["status1"]=data2$status

#data2["status1"] = lapply(data2$status, as.character)

data2["status2"] = 0

data2$status2[which(data2$status1==' 1 ')] <- 1

data2$status2[which(data2$status1==' 2 ')] <- 1

data2$status2 = as.numeric(data2$status2)

#продолжительность рабочей недели

dur1 = as.character(data2$mj6.2)

dur2 = lapply(dur1, as.integer)

dur3 = as.numeric(unlist(dur2))

data2["dur"] = (dur3 - mean(dur3)) / sqrt(var(dur3))

#семейное положение

data2["wed"]= data2$m_marst

data2$wed1 = 0

data2$wed1[which(data2$wed==' 2 ')] <- 1

data2$wed1 = as.numeric(data2$wed1)

data2["wed2"] = lapply(data2["wed"], as.character)

data2$wed2 = 0

data2$wed2[which(data2$wed==' 4 ')] <- 1

data2$wed2[which(data2$wed==' 5 ')] <- 1

data2$wed2 = as.numeric(data2$wed2)

data2["wed3"]=data2$m_marst

data2$wed3 = 0

data2$wed3[which(data2$wed==' 1 ')] <- 1

data2$wed3 = as.numeric(data2$wed3)

data2 = na.omit(data2)

#Подмножество : С высшим образованием, не из города

data3=subset(data2,higher_educ== 1 )

data4=subset(data3,status2== 0 )

data4

model_top = lm(data = data3, (salary+ 3 )^(0.1)~I((age+ 3 )^(0.1)) + sex + 1 +
status2+I((dur+ 1 )^(0.1))+I(age*dur)+wed1+wed2+wed3)

summary(model_top)

vif(model_top)#большой vif у wed1 , потому здесь его можно убрать и на результат
он не повлияет

#adj R^2 = 0.06632=6,632% , параметры I((age+3)^(0.1))+I((dur+1)^(0.1))+I(age*dur)
значимые

##Вывод : В подмножестве( С высшим образованием, не из города) хорошо
зарабатывают более взрослые респонденты

#с более длинной рабочей неделей

#Подмножество : женщины с высшим образованием

data5=subset(data2,higher_educ== 1 )

data5=subset(data5,sex== 0 )

model_top = lm(data = data5, (salary+ 3 )^(0.1)~I((age+ 3 )^(0.1)) + 0 + 1 +
status2+I((dur+ 1 )^(0.1))+I(age*dur)+wed1+wed2+wed3)

summary(model_top)

vif(model_top)#большой vif у wed1 , потому здесь его можно убрать и на результат
он не повлияет

#adj R^2 = 0.07919=7,919% , параметры I((age+3)^(0.1))+I((dur+1)^(0.1))+I(age*dur)
значимые

#Вывод в подмножестве( С женщины с высшим образованием) хорошо
зарабатывают более взрослые респонденты

#с более длинной рабочей неделей

Приложение 4.

DecisionTreeClassifier

Приложение 5.

Приложение 6.
